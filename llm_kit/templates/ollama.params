# ===== ОСНОВНЫЕ НАСТРОЙКИ LLM =====

url = http://localhost:11434/api/chat                   # API-эндпоинт сервиса. Вы можете указать адрес сервера вместо localhost
model = укажите_название_модели                         # Название модели (например: llama3.3)

# ===== ДОПОЛНИТЕЛЬНЫЕ ПАРАМЕТРЫ =====

seed = 42                                               # Установка seed для воспроизводимости генерации ответа
temperature = 0.7                                       # Температура (0.0-2.0): ниже — строже и предсказуемее, выше — больше креатива и разнообразия
num_predict = 1000                                      # Максимальное количество токенов в ответе (-1 = бесконечная генерация, -2 = заполнить контекст)

# ===== РЕКОМЕНДУЕМЫЕ (ОПЦИОНАЛЬНЫЕ) ПАРАМЕТРЫ =====

# top_p = 0.9                                           # Параметр ядерной выборки (0.0-1.0): учитывает только топ токены с суммарной вероятностью P
# top_k = 40                                            # Ограничение выбора топ-K токенов: 0 отключает ограничение, большее число = более разнообразные ответы
# repeat_penalty = 1.1                                  # Штраф за повторения (0.0-2.0): чем выше, тем меньше повторений токенов
# presence_penalty = 0.0                                # Штраф за присутствие токенов: снижает вероятность повторения уже встречавшихся слов
# frequency_penalty = 0.0                               # Частотный штраф: штрафует токены пропорционально частоте их появления в тексте
# min_p = 0.0                                           # Минимальная вероятность токена (0.0-1.0): фильтрует токены с низкой относительной вероятностью
# typical_p = 1.0                                       # Типичная выборка (0.0-1.0): приоритет токенам с "типичной" вероятностью для контекста
# tfs_z = 1.0                                           # Хвостовая свободная выборка: уменьшает влияние маловероятных токенов (1.0 отключает)
# num_ctx = 2048                                        # Размер контекстного окна: количество токенов, которые модель может учитывать
# num_keep = 4                                          # Количество токенов для сохранения в начале: не будут удалены при превышении контекста
# repeat_last_n = 64                                    # Окно для контроля повторений: сколько последних токенов учитывать для штрафа за повторения
# stop = []                                             # Массив стоп-слов: генерация прекратится при встрече любого из этих токенов
# stream = true                                         # Потоковая передача ответа: true для получения ответа частями, false для полного ответа
# format = json                                         # Формат ответа: json для структурированного JSON-ответа
# keep_alive = 5m                                       # Время хранения модели в памяти: 5m = 5 минут, 0 = выгрузить сразу

# ===== РЕДКИЕ/СПЕЦИАЛЬНЫЕ ПАРАМЕТРЫ Ollama =====

# mirostat = 0                                          # Алгоритм Mirostat для контроля перплексии: 0 отключен, 1 = Mirostat, 2 = Mirostat 2.0
# mirostat_tau = 5.0                                    # Целевая энтропия для Mirostat: контролирует баланс связности и разнообразия
# mirostat_eta = 0.1                                    # Скорость обучения Mirostat: как быстро алгоритм адаптируется к обратной связи
# penalize_newline = true                               # Штраф за переносы строк: снижает вероятность генерации лишних переносов строк
# numa = false                                          # Использование NUMA архитектуры: оптимизация для многопроцессорных систем
# num_batch = 512                                       # Размер батча для обработки промптов: большее значение = быстрее, но больше памяти
# num_gpu = 1                                           # Количество GPU слоев: -1 = определить автоматически, 0 = только CPU, >0 = количество слоев на GPU
# main_gpu = 0                                          # Основной GPU для вычислений: ID GPU, который будет использоваться для небольших тензоров
# low_vram = false                                      # Режим низкого потребления видеопамяти: экономит VRAM, но может замедлить работу
# f16_kv = true                                         # Использование 16-битных ключей и значений: экономит память с минимальной потерей качества
# logits_all = false                                    # Возврат логитов для всех токенов: true возвращает вероятности всех токенов, не только последнего
# vocab_only = false                                    # Загрузка только словаря: true загружает только токенизатор без весов модели
# use_mmap = true                                       # Использование memory mapping: загружает модель по частям при необходимости
# use_mlock = false                                     # Блокировка модели в RAM: предотвращает выгрузку модели в swap, требует больше RAM
# num_thread = 0                                        # Количество потоков CPU: 0 = автоопределение, рекомендуется количество физических ядер
# raw = false                                           # Сырой режим промпта: true отключает применение шаблона промпта модели
# system = ""                                           # Системное сообщение: переопределяет системный промпт модели
# template = ""                                         # Шаблон промпта: переопределяет стандартный шаблон форматирования промпта модели
# context = []                                          # Контекст предыдущего запроса: массив токенов для продолжения беседы
# images = []                                           # Массив изображений в base64: для мультимодальных моделей (например, llava)
# tools = []                                            # Инструменты для вызова функций: описания функций, которые может вызывать модель